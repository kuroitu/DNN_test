{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行について\n",
    "上から順に実験コード前まで実行していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目次\n",
    "\n",
    "- [誤差関数](#誤差関数)\n",
    "- [活性化関数](#活性化関数)\n",
    "- [最適化](#最適化)\n",
    "- [CNN util](#CNN-util)\n",
    "- [レイヤ](#レイヤ)\n",
    "- [レイヤマネージャ](#レイヤマネージャ)\n",
    "- [実験コード](#実験コード)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差関数\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Error():\n",
    "    def __init__(self, *args, **kwds):\n",
    "        self.error = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, *args, **kwds):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def backward(self, *args, **kwds):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def total_error(self, *args, **kwds):\n",
    "        return np.sum(self.error)/self.error.size\n",
    "\n",
    "\n",
    "class SquareError(Error):\n",
    "    def forward(self, y, t, *args, **kwds):\n",
    "        self.error = 0.5 * (y - t)**2\n",
    "        return self.error\n",
    "    \n",
    "    \n",
    "    def backward(self, y, t, *args, **kwds):\n",
    "        return y - t\n",
    "\n",
    "\n",
    "class BinaryCrossEntropy(Error):\n",
    "    def forward(self, y, t, *args, eps=1e-8, **kwds):\n",
    "        self.error = - t*np.log(y+eps) - (1 - t)*np.log(1-y+eps)\n",
    "        return self.error\n",
    "    \n",
    "    \n",
    "    def backward(self, y, t, *args, eps=1e-8, **kwds):\n",
    "        return (y - t) / (y*(1 - y) + eps)\n",
    "    \n",
    "\n",
    "class CrossEntropy(Error):\n",
    "    def forward(self, y, t, *args, eps=1e-8, **kwds):\n",
    "        self.error = - t*np.log(y+eps)\n",
    "        return self.error\n",
    "    \n",
    "    \n",
    "    def backward(self, y, t, *args, eps=1e-8, **kwds):\n",
    "        return - t/(y+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_err_dic = {\"Square\": SquareError,\n",
    "            \"Binary\": BinaryCrossEntropy,\n",
    "            \"Cross\": CrossEntropy,\n",
    "           }\n",
    "\n",
    "\n",
    "def get_err(name, *args, **kwds):\n",
    "    if name in _err_dic.keys():\n",
    "        errfunc = _err_dic[name](*args, **kwds)\n",
    "    else:\n",
    "        raise ValueError(name + \": Unknown error function\")\n",
    "\n",
    "    return errfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Activator():\n",
    "    def __init__(self, *args, **kwds):\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwds):\n",
    "        raise NotImplemented\n",
    "        #raise Exception(\"Not Implemented\")\n",
    "\n",
    "\n",
    "    def backward(self, *args, **kwds):\n",
    "        raise NotImplemented\n",
    "        #raise Exception(\"Not Implemented\")\n",
    "\n",
    "\n",
    "    def update(self, *args, **kwds):\n",
    "        raise NotImplemented\n",
    "        #pass\n",
    "\n",
    "\n",
    "class step(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.zeros_like(x)\n",
    "\n",
    "\n",
    "class identity(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "\n",
    "class bentIdentity(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return 0.5*(np.sqrt(x**2 + 1) - 1) + x\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return 0.5*x/np.sqrt(x**2 + 1) + 1\n",
    "\n",
    "\n",
    "class hardShrink(Activator):\n",
    "    def __init__(self, lambda_=0.5, *args, **kwds):\n",
    "        self.lambda_ = lambda_\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where((-self.lambda_ <= x) & (x <= self.lambda_), 0, x)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where((-self.lambda_ <= x) & (x <= self.lambda_), 0, 1)\n",
    "\n",
    "\n",
    "class softShrink(Activator):\n",
    "    def __init__(self, lambda_=0.5, *args, **kwds):\n",
    "        self.lambda_ = lambda_\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x < -self.lambda_, x + self.lambda_,\n",
    "                        np.where(x > self.lambda_, x - self.lambda_, 0))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where((-self.lambda_ <= x) & (x <= self.lambda_), 0, 1)\n",
    "\n",
    "\n",
    "class threshold(Activator):\n",
    "    def __init__(self, threshold, value, *args, **kwds):\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x > self.threshold, x, self.value)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x > self.threshold, 1, 0)\n",
    "\n",
    "\n",
    "class sigmoid(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def backward(self, x, y, *args, **kwds):\n",
    "        return y*(1 - y)\n",
    "\n",
    "\n",
    "class hardSigmoid(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.clip(0.2*x + 0.5, 0, 1)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where((x > 2.5) | (x < -2.5), 0, 0.2)\n",
    "\n",
    "\n",
    "class logSigmoid(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return -np.log(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return 1/(1 + np.exp(x))\n",
    "\n",
    "\n",
    "class act_tanh(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.tanh(x)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "class hardtanh(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.clip(x, -1, 1)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where((-1 <= x) & (x <= 1), 1, 0)\n",
    "\n",
    "\n",
    "class tanhShrink(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x - np.tanh(x)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.tanh(x)**2\n",
    "\n",
    "\n",
    "class ReLU(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "\n",
    "class ReLU6(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.clip(x, 0, 6)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where((0 < x) & (x < 6), 1, 0)\n",
    "\n",
    "\n",
    "class leakyReLU(Activator):\n",
    "    def __init__(self, alpha=1e-2, *args, **kwds):\n",
    "        self.alpha = alpha\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.maximum(self.alpha * x, x)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x < 0, self.alpha, 1)\n",
    "\n",
    "\n",
    "class ELU(Activator):\n",
    "    def __init__(self, alpha=1., *args, **kwds):\n",
    "        self.alpha = alpha\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, x, self.alpha*(np.exp(x) - 1))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, 1, self.alpha*np.exp(x))\n",
    "\n",
    "\n",
    "class SELU(Activator):\n",
    "    def __init__(self, lambda_=1.0507, alpha=1.67326, *args, **kwds):\n",
    "        self.lambda_ = lambda_\n",
    "        self.alpha = alpha\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, self.lambda_*x,\n",
    "                        self.lambda_*self.alpha*(np.exp(x)-1))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, self.lambda_, self.lambda_*self.alpha*np.exp(x))\n",
    "\n",
    "\n",
    "class CELU(Activator):\n",
    "    def __init__(self, alpha=1., *args, **kwds):\n",
    "        self.alpha = alpha\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, x, self.alpha*(np.exp(x/self.alpha)-1))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return np.where(x >= 0, 1, np.exp(x/self.alpha))\n",
    "\n",
    "\n",
    "class softmax(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        exp_x = np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x/np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def backward(self, x, y, *args, **kwds):\n",
    "        return y*(1 - y)\n",
    "\n",
    "\n",
    "class softmin(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        exp_mx = np.exp(-x)\n",
    "        return exp_mx/np.sum(exp_mx, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def backward(self, x, y, *args, **kwds):\n",
    "        return -y*(1 - y)\n",
    "\n",
    "\n",
    "class logSoftmax(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        exp_x = np.exp(x)\n",
    "        return np.log(exp_x/np.sum(exp_x, axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "    def backward(self, x, y, *args, **kwds):\n",
    "        return 1 - np.exp(y)\n",
    "\n",
    "\n",
    "class softplus(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return np.logaddexp(x, 0)\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class softsign(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x/(1 + np.abs(x))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        return 1/(1 + np.abs(x))**2\n",
    "\n",
    "\n",
    "class Swish(Activator):\n",
    "    def __init__(self, beta=1, *args, **kwds):\n",
    "        self.beta = beta\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x/(1 + np.exp(-self.beta*x))\n",
    "\n",
    "\n",
    "    def backward(self, x, y, *args, **kwds):\n",
    "        return self.beta*y + (1 - self.beta*y)/(1 + np.exp(-self.beta*x))\n",
    "\n",
    "\n",
    "    def d2y(self, x, *args, **kwds):\n",
    "        return (-0.25*self.beta*(self.beta*x*np.tanh(0.5*self.beta*x) - 2)\n",
    "                               *(1 - np.tanh(0.5*self.beta*x)**2))\n",
    "\n",
    "\n",
    "class Mish(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x*np.tanh(np.logaddexp(x, 0))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        omega = (4*(x + 1) + 4*np.exp(2*x)\n",
    "              + np.exp(3*x) + (4*x + 6)*np.exp(x))\n",
    "        delta = 2*np.exp(x) + np.exp(2*x) + 2\n",
    "        return np.exp(x)*omega/delta**2\n",
    "\n",
    "\n",
    "    def d2y(self, x, *args, **kwds):\n",
    "        omega = (2*(x + 2)\n",
    "                 + np.exp(x)*(np.exp(x)*(-2*np.exp(x)*(x - 1) - 3*x + 6)\n",
    "                              + 2*(x + 4)))\n",
    "        delta = np.exp(x)*(np.exp(x) + 2) + 2\n",
    "        return 4*np.exp(x)*omega/delta**3\n",
    "\n",
    "\n",
    "class tanhExp(Activator):\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        return x*np.tanh(np.exp(x))\n",
    "\n",
    "\n",
    "    def backward(self, x, *args, **kwds):\n",
    "        tanh_exp_x = np.tanh(np.exp(x))\n",
    "        return tanh_exp_x - x*np.exp(x)*(tanh_exp_x**2 - 1)\n",
    "\n",
    "\n",
    "    def d2y(self, x, *args, **kwds):\n",
    "        tanh_exp = np.tanh(np.exp(x))\n",
    "        return (np.exp(x)*(-x + 2*np.exp(x)*x*tanh_exp - 2)\n",
    "                         *(tanh_exp**2 - 1))\n",
    "\n",
    "\n",
    "class maxout(Activator):\n",
    "    def __init__(self, n_prev, n, k, wb_width=5e-2, *args, **kwds):\n",
    "        self.n_prev = n_prev\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.w = wb_width*np.random.rand((n_prev, n*k))\n",
    "        self.b = wb_width*np.random.rand(n*k)\n",
    "\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "\n",
    "    def forward(self, x, *args, **kwds):\n",
    "        self.x = x.copy()\n",
    "        self.z = np.dot(self.w.T, x) + self.b\n",
    "        self.z = self.z.reshape(self.n, self.k)\n",
    "        self.y = np.max(self.z, axis=1)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, g, *args, **kwds):\n",
    "        self.dw = np.sum(np.dot(self.w, self.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_act_dic = {\"step\": step,\n",
    "            \"identity\": identity,\n",
    "            \"bent-identity\": bentIdentity,\n",
    "            \"hard-shrink\": hardShrink,\n",
    "            \"soft-shrink\": softShrink,\n",
    "            \"threshold\": threshold,\n",
    "            \"sigmoid\": sigmoid,\n",
    "            \"hard-sigmoid\": hardSigmoid,\n",
    "            \"log-sigmoid\": logSigmoid,\n",
    "            \"tanh\": act_tanh,\n",
    "            \"tanh-shrink\": tanhShrink,\n",
    "            \"hard-tanh\":hardtanh,\n",
    "            \"ReLU\": ReLU,\n",
    "            \"ReLU6\": ReLU6,\n",
    "            \"leaky-ReLU\": leakyReLU,\n",
    "            \"ELU\": ELU,\n",
    "            \"SELU\": SELU,\n",
    "            \"CELU\": CELU,\n",
    "            \"softmax\": softmax,\n",
    "            \"softmin\": softmin,\n",
    "            \"log-softmax\": logSoftmax,\n",
    "            \"softplus\": softplus,\n",
    "            \"softsign\": softsign,\n",
    "            \"Swish\": Swish,\n",
    "            \"Mish\": Mish,\n",
    "            \"tanhExp\": tanhExp,\n",
    "           }\n",
    "\n",
    "\n",
    "def get_act(name, *args, **kwds):\n",
    "    if name in _act_dic.keys():\n",
    "        activator = _act_dic[name](*args, **kwds)\n",
    "    else:\n",
    "        raise ValueError(name + \": Unknown activator\")\n",
    "\n",
    "    return activator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Optimizer():\n",
    "    \"\"\"\n",
    "    最適化手法が継承するスーパークラス。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwds):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def update(self, *args, **kwds):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, eta=1e-2, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        dw = -self.eta*grad_w\n",
    "        db = -self.eta*grad_b\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class MSGD(Optimizer):\n",
    "    def __init__(self, eta=1e-2, mu=0.9, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.dw = 1e-8\n",
    "        self.db = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        dw = self.mu*self.dw - (1-self.mu)*self.eta*grad_w\n",
    "        db = self.mu*self.db - (1-self.mu)*self.eta*grad_b\n",
    "\n",
    "        # コピーではなくビューで代入しているのは、これらの値が使われることはあっても\n",
    "        # 変更されることはないためです。\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class NAG(Optimizer):\n",
    "    def __init__(self, eta=1e-2, mu=0.9, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.mu = mu\n",
    "\n",
    "        # 一つ前のステップの値を保持\n",
    "        self.dw = 1e-8\n",
    "        self.db = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, w=0, b=0, dfw=None, dfb=None,\n",
    "               nargs=2, *args, **kwds):\n",
    "        if nargs == 1:\n",
    "            grad_w = dfw(w + self.mu*self.dw)\n",
    "            grad_b = 1e-8\n",
    "        elif nargs == 2:\n",
    "            grad_w = dfw(w + self.mu*self.dw, b + self.mu*self.db)\n",
    "            grad_b = dfb(w + self.mu*self.dw, b + self.mu*self.db)\n",
    "\n",
    "        dw = self.mu*self.dw - (1-self.mu)*self.eta*grad_w\n",
    "        db = self.mu*self.db - (1-self.mu)*self.eta*grad_b\n",
    "\n",
    "        # コピーではなくビューで代入しているのは、これらの値が使われることはあっても\n",
    "        # 変更されることはないためです。\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, eta=1e-3, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.gw = 1e-8\n",
    "        self.gb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        self.gw += grad_w*grad_w\n",
    "        self.gb += grad_b*grad_b\n",
    "\n",
    "        dw = -self.eta*grad_w/np.sqrt(self.gw)\n",
    "        db = -self.eta*grad_b/np.sqrt(self.gb)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, eta=1e-2, rho=0.99, eps=1e-8, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        self.vw += (1-self.rho)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.rho)*(grad_b**2 - self.vb)\n",
    "\n",
    "        dw = -self.eta*grad_w/np.sqrt(self.vw+self.eps)\n",
    "        db = -self.eta*grad_b/np.sqrt(self.vb+self.eps)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AdaDelta(Optimizer):\n",
    "    def __init__(self, rho=0.95, eps=1e-6, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.uw = 1e-8\n",
    "        self.ub = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        self.vw += (1-self.rho)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.rho)*(grad_b**2 - self.vb)\n",
    "\n",
    "        dw = -grad_w*np.sqrt(self.uw+self.eps)/np.sqrt(self.vw+self.eps)\n",
    "        db = -grad_b*np.sqrt(self.ub+self.eps)/np.sqrt(self.vb+self.eps)\n",
    "\n",
    "        self.uw += (1-self.rho)*(dw**2 - self.uw)\n",
    "        self.ub += (1-self.rho)*(db**2 - self.ub)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, alpha=1e-3, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                 *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.beta2)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.beta2)*(grad_b**2 - self.vb)\n",
    "\n",
    "        alpha_t = self.alpha*np.sqrt(1-self.beta2**t)/(1-self.beta1**t)\n",
    "\n",
    "        dw = -alpha_t*self.mw/(np.sqrt(self.vw+self.eps))\n",
    "        db = -alpha_t*self.mb/(np.sqrt(self.vb+self.eps))\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class RMSpropGraves(Optimizer):\n",
    "    def __init__(self, eta=1e-4, rho=0.95, eps=1e-4, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self,grad_w, grad_b, *args, **kwds):\n",
    "        self.mw += (1-self.rho)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.rho)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.rho)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.rho)*(grad_b**2 - self.vb)\n",
    "\n",
    "        dw = -self.eta*grad_w/np.sqrt(self.vw - self.mw**2 + self.eps)\n",
    "        db = -self.eta*grad_b/np.sqrt(self.vb - self.mb**2 + self.eps)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class SMORMS3(Optimizer):\n",
    "    def __init__(self, eta=1e-3, eps=1e-8, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.zetaw = 1e-8\n",
    "        self.zetab = 1e-8\n",
    "        self.sw = 1\n",
    "        self.sb = 1\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, *args, **kwds):\n",
    "        rhow = 1/(1+self.sw)\n",
    "        rhob = 1/(1+self.sb)\n",
    "\n",
    "        self.mw += (1-rhow)*(grad_w - self.mw)\n",
    "        self.mb += (1-rhob)*(grad_b - self.mb)\n",
    "        self.vw += (1-rhow)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-rhob)*(grad_b**2 - self.vb)\n",
    "\n",
    "        self.zetaw = self.mw**2 / (self.vw + self.eps)\n",
    "        self.zetaw = self.mb**2 / (self.vb + self.eps)\n",
    "\n",
    "        dw = -grad_w*(np.minimum(self.eta, self.zetaw)\n",
    "                      /np.sqrt(self.vw + self.eps))\n",
    "        db = -grad_b*(np.minimum(self.eta, self.zetab)\n",
    "                      /np.sqrt(self.vb + self.eps))\n",
    "\n",
    "        self.sw = 1 + (1 - self.zetaw)*self.sw\n",
    "        self.sb = 1 + (1 - self.zetab)*self.sb\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AdaMax(Optimizer):\n",
    "    def __init__(self, alpha=2e-3, beta1=0.9, beta2=0.999,\n",
    "                 *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.uw = 1e-8\n",
    "        self.ub = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "        self.uw = np.maximum(self.beta2*self.uw, np.abs(grad_w))\n",
    "        self.ub = np.maximum(self.beta2*self.ub, np.abs(grad_b))\n",
    "\n",
    "        alpha_t = self.alpha/(1 - self.beta1**t)\n",
    "\n",
    "        dw = -alpha_t*self.mw/self.uw\n",
    "        db = -alpha_t*self.mb/self.ub\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class Nadam(Optimizer):\n",
    "    def __init__(self, alpha=2e-3, mu=0.975, nu=0.999, eps=1e-8,\n",
    "                 *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.mu = mu\n",
    "        self.nu = nu\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.mu)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.mu)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.nu)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.nu)*(grad_b**2 - self.vb)\n",
    "\n",
    "        mhatw = (self.mu*self.mw/(1-self.mu**(t+1))\n",
    "                 + (1-self.mu)*grad_w/(1-self.mu**t))\n",
    "        mhatb = (self.mu*self.mb/(1-self.mu**(t+1))\n",
    "                 + (1-self.mu)*grad_b/(1-self.mu**t))\n",
    "        vhatw = self.nu*self.vw/(1-self.nu**t)\n",
    "        vhatb = self.nu*self.vb/(1-self.nu**t)\n",
    "\n",
    "        dw = -self.alpha*mhatw/np.sqrt(vhatw + self.eps)\n",
    "        db = -self.alpha*mhatb/np.sqrt(vhatb + self.eps)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class Eve(Optimizer):\n",
    "    def __init__(self, alpha=1e-3, beta1=0.9, beta2=0.999, beta3=0.999,\n",
    "                 c=10, eps=1e-8, fstar=0, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "        self.c = c\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.f = 0\n",
    "        self.fstar = fstar\n",
    "        self.dtilde_w = 1e-8\n",
    "        self.dtilde_b = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, f=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.beta2)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.beta2)*(grad_b**2 - self.vb)\n",
    "\n",
    "        mhatw = self.mw/(1 - self.beta1**t)\n",
    "        mhatb = self.mb/(1 - self.beta1**t)\n",
    "        vhatw = self.vw/(1 - self.beta2**t)\n",
    "        vhatb = self.vb/(1 - self.beta2**t)\n",
    "\n",
    "        if t > 1:\n",
    "            d_w = (np.abs(f-self.fstar)\n",
    "                    /(np.minimum(f, self.f) - self.fstar))\n",
    "            d_b = (np.abs(f-self.fstar)\n",
    "                    /(np.minimum(f, self.f) - self.fstar))\n",
    "            dhat_w = np.clip(d_w, 1/self.c, self.c)\n",
    "            dhat_b = np.clip(d_b, 1/self.c, self.c)\n",
    "            self.dtilde_w += (1 - self.beta3)*(dhat_w - self.dtilde_w)\n",
    "            self.dtilde_b += (1 - self.beta3)*(dhat_b - self.dtilde_b)\n",
    "        else:\n",
    "            self.dtilde_w = 1\n",
    "            self.dtilde_b = 1\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "        dw = -(self.alpha*mhatw\n",
    "               /(self.dtilde_w*(np.sqrt(vhatw) + self.eps)))\n",
    "        db = -(self.alpha*mhatb\n",
    "               /(self.dtilde_b*(np.sqrt(vhatb) + self.eps)))\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class SantaE(Optimizer):\n",
    "    def __init__(self, eta=1e-2, sigma=0.95, lambda_=1e-8,\n",
    "                 anne_func=lambda t, n: t**n, anne_rate=0.5,\n",
    "                 burnin=100, C=5, N=16,\n",
    "                 *args, **kwds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eta: Learning rate\n",
    "            sigma: Maybe in other cases;\n",
    "                    'rho' in RMSprop, AdaDelta, RMSpropGraves.\n",
    "                    'rhow' or 'rhob' in SMORMS3.\n",
    "                    'beta2' in Adam, Eve.\n",
    "                    'nu' in Nadam.\n",
    "                   To use calculation 'v'.\n",
    "            lambda_: Named 'eps'(ε) in other cases.\n",
    "            anne_func: Annealing function.\n",
    "                       To use calculation 'beta' at each timestep.\n",
    "                       Default is 'timestep'**'annealing rate'.\n",
    "                       The calculated value should be towards infinity\n",
    "                       as 't' increases.\n",
    "            anne_rate: Annealing rate.\n",
    "                       To use calculation 'beta' at each timestep.\n",
    "                       The second Argument of 'anne_func'.\n",
    "            burnin: Swith exploration and refinement.\n",
    "                    This should be specified by users.\n",
    "            C: To calculate first 'alpha'.\n",
    "            N: Number of minibatch.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.sigma = sigma\n",
    "        self.lambda_ = lambda_\n",
    "        self.anne_func = anne_func\n",
    "        self.anne_rate = anne_rate\n",
    "        self.burnin = burnin\n",
    "        self.N = N\n",
    "\n",
    "        # Keep one step before and Initialize.\n",
    "        self.alpha_w = np.sqrt(eta)*C\n",
    "        self.alpha_b = np.sqrt(eta)*C\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.gw = 1e-8\n",
    "        self.gb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        try:\n",
    "            shape_w = grad_w.shape\n",
    "        except:\n",
    "            shape_w = (1, )\n",
    "        try:\n",
    "            shape_b = grad_b.shape\n",
    "        except:\n",
    "            shape_b = (1, )\n",
    "\n",
    "        if t == 1:\n",
    "            # Initialize uw, ub.\n",
    "            self.uw = np.sqrt(self.eta)*np.random.randn(*shape_w)\n",
    "            self.ub = np.sqrt(self.eta)*np.random.randn(*shape_b)\n",
    "\n",
    "        self.vw = (self.sigma*self.vw\n",
    "                   + grad_w*grad_w * (1 - self.sigma) / self.N**2)\n",
    "        self.vb = (self.sigma*self.vb\n",
    "                   + grad_b*grad_b * (1 - self.sigma) / self.N**2)\n",
    "\n",
    "        gw = 1/np.sqrt(self.lambda_ + np.sqrt(self.vw))\n",
    "        gb = 1/np.sqrt(self.lambda_ + np.sqrt(self.vb))\n",
    "\n",
    "        beta = self.anne_func(t, self.anne_rate)\n",
    "        if t < self.burnin:\n",
    "            # Exploration.\n",
    "            self.alpha_w += self.uw*self.uw - self.eta/beta\n",
    "            self.alpha_b += self.ub*self.ub - self.eta/beta\n",
    "\n",
    "            uw = (self.eta/beta * (1 - self.gw/gw)/self.uw\n",
    "                  + np.sqrt(2*self.eta/beta * self.gw)\n",
    "                  * np.random.randn(*shape_w))\n",
    "            ub = (self.eta/beta * (1 - self.gb/gb)/self.ub\n",
    "                  + np.sqrt(2*self.eta/beta * self.gb)\n",
    "                  * np.random.randn(*shape_b))\n",
    "        else:\n",
    "            # Refinement.\n",
    "            uw = 0\n",
    "            ub = 0\n",
    "\n",
    "        uw += (1 - self.alpha_w)*self.uw - self.eta*gw*grad_w\n",
    "        ub += (1 - self.alpha_b)*self.ub - self.eta*gb*grad_b\n",
    "\n",
    "        # Update values.\n",
    "        self.uw = uw\n",
    "        self.ub = ub\n",
    "        self.gw = gw\n",
    "        self.gb = gb\n",
    "\n",
    "        dw = gw*uw\n",
    "        db = gb*ub\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class SantaSSS(Optimizer):\n",
    "    def __init__(self, eta=1e-2, sigma=0.95, lambda_=1e-8,\n",
    "                 anne_func=lambda t, n: t**n, anne_rate=0.5,\n",
    "                 burnin=100, C=5, N=16,\n",
    "                 *args, **kwds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eta: Learning rate\n",
    "            sigma: Maybe in other cases;\n",
    "                    'rho' in RMSprop, AdaDelta, RMSpropGraves.\n",
    "                    'rhow' or 'rhob' in SMORMS3.\n",
    "                    'beta2' in Adam, Eve.\n",
    "                    'nu' in Nadam.\n",
    "                   To use calculation 'v'.\n",
    "            lambda_: Named 'eps'(ε) in other cases.\n",
    "            anne_func: Annealing function.\n",
    "                       To use calculation 'beta' at each timestep.\n",
    "                       Default is 'timestep'**'annealing rate'.\n",
    "                       The calculated value should be towards infinity\n",
    "                       as 't' increases.\n",
    "            anne_rate: Annealing rate.\n",
    "                       To use calculation 'beta' at each timestep.\n",
    "                       The second Argument of 'anne_func'.\n",
    "            burnin: Swith exploration and refinement.\n",
    "                    This should be specified by users.\n",
    "            C: To calculate first 'alpha'.\n",
    "            N: Number of minibatch.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.eta = eta\n",
    "        self.sigma = sigma\n",
    "        self.lambda_ = lambda_\n",
    "        self.anne_func = anne_func\n",
    "        self.anne_rate = anne_rate\n",
    "        self.burnin = burnin\n",
    "        self.N = N\n",
    "\n",
    "        # Keep one step before and Initialize.\n",
    "        self.alpha_w = np.sqrt(eta)*C\n",
    "        self.alpha_b = np.sqrt(eta)*C\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.gw = 1e-8\n",
    "        self.gb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        try:\n",
    "            shape_w = grad_w.shape\n",
    "        except:\n",
    "            shape_w = (1, )\n",
    "        try:\n",
    "            shape_b = grad_b.shape\n",
    "        except:\n",
    "            shape_b = (1, )\n",
    "\n",
    "        if t == 1:\n",
    "            # Initialize uw, ub.\n",
    "            self.uw = np.sqrt(self.eta)*np.random.randn(*shape_w)\n",
    "            self.ub = np.sqrt(self.eta)*np.random.randn(*shape_b)\n",
    "\n",
    "        self.vw = (self.sigma*self.vw\n",
    "                   + grad_w*grad_w * (1 - self.sigma) / self.N**2)\n",
    "        self.vb = (self.sigma*self.vb\n",
    "                   + grad_b*grad_b * (1 - self.sigma) / self.N**2)\n",
    "\n",
    "        gw = 1/np.sqrt(self.lambda_ + np.sqrt(self.vw))\n",
    "        gb = 1/np.sqrt(self.lambda_ + np.sqrt(self.vb))\n",
    "\n",
    "        dw = 0.5*gw*self.uw\n",
    "        db = 0.5*gb*self.ub\n",
    "\n",
    "        beta = self.anne_func(t, self.anne_rate)\n",
    "        if t < self.burnin:\n",
    "            # Exploration.\n",
    "            self.alpha_w += (self.uw*self.uw - self.eta/beta)*0.5\n",
    "            self.alpha_b += (self.ub*self.ub - self.eta/beta)*0.5\n",
    "\n",
    "            uw = np.exp(-0.5*self.alpha_w)*self.uw\n",
    "            ub = np.exp(-0.5*self.alpha_b)*self.ub\n",
    "            uw += (-gw*grad_w*self.eta\n",
    "                        + np.sqrt(2*self.gw*self.eta/beta)\n",
    "                        * np.random.randn(*shape_w)\n",
    "                        + self.eta/beta*(1-self.gw/gw)/self.uw)\n",
    "            ub += (-gb*grad_b*self.eta\n",
    "                        + np.sqrt(2*self.gb*self.eta/beta)\n",
    "                        * np.random.randn(*shape_b)\n",
    "                        + self.eta/beta*(1-self.gb/gb)/self.ub)\n",
    "            uw *= np.exp(-0.5*self.alpha_w)\n",
    "            ub *= np.exp(-0.5*self.alpha_b)\n",
    "\n",
    "            self.alpha_w += (uw*uw - self.eta/beta)*0.5\n",
    "            self.alpha_b += (ub*ub - self.eta/beta)*0.5\n",
    "        else:\n",
    "            # Refinement.\n",
    "            uw = np.exp(-0.5*self.alpha_w)*self.uw\n",
    "            ub = np.exp(-0.5*self.alpha_b)*self.ub\n",
    "\n",
    "            uw -= gw*grad_w*self.eta\n",
    "            ub -= gb*grad_b*self.eta\n",
    "\n",
    "            uw *= np.exp(-0.5*self.alpha_w)\n",
    "            ub *= np.exp(-0.5*self.alpha_b)\n",
    "\n",
    "        # Update values.\n",
    "        self.uw = uw\n",
    "        self.ub = ub\n",
    "        self.gw = gw\n",
    "        self.gb = gb\n",
    "\n",
    "        dw = gw*uw*0.5\n",
    "        db = gb*ub*0.5\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AMSGrad(Optimizer):\n",
    "    def __init__(self, alpha=1e-3, beta1=0.9, beta2=0.999, eps=1e-8,\n",
    "                 *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.vhatw = 1e-8\n",
    "        self.vhatb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "\n",
    "        self.vw += (1-self.beta2)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.beta2)*(grad_b**2 - self.vb)\n",
    "\n",
    "        self.vhatw = np.maximum(self.vhatw, self.vw)\n",
    "        self.vhatb = np.maximum(self.vhatb, self.vb)\n",
    "\n",
    "        alpha_t = self.alpha / np.sqrt(t)\n",
    "\n",
    "        dw = - alpha_t * self.mw/np.sqrt(self.vhatw + self.eps)\n",
    "        db = - alpha_t * self.mb/np.sqrt(self.vhatb + self.eps)\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AdaBound(Optimizer):\n",
    "    def __init__(self, alpha=1e-3, eta=1e-1, beta1=0.9, beta2=0.999,\n",
    "                 eps=1e-8, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.beta2)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.beta2)*(grad_b**2 - self.vb)\n",
    "\n",
    "        etal = self.eta*(1 - 1/((1-self.beta2)*t + 1))\n",
    "        etau = self.eta*(1 + 1/((1-self.beta2)*t + self.eps))\n",
    "\n",
    "        etahatw_t = np.clip(self.alpha/np.sqrt(self.vw), etal, etau)\n",
    "        etahatb_t = np.clip(self.alpha/np.sqrt(self.vb), etal, etau)\n",
    "\n",
    "        etaw_t = etahatw_t/np.sqrt(t)\n",
    "        etab_t = etahatb_t/np.sqrt(t)\n",
    "\n",
    "        dw = - etaw_t*self.mw\n",
    "        db = - etab_t*self.mb\n",
    "\n",
    "        return dw, db\n",
    "\n",
    "\n",
    "class AMSBound(Optimizer):\n",
    "    def __init__(self, alpha=1e-3, eta=1e-1, beta1=0.9, beta2=0.999,\n",
    "                 eps=1e-8, *args, **kwds):\n",
    "        super().__init__(*args, **kwds)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        # 一つ前のステップの値を保持する\n",
    "        self.mw = 1e-8\n",
    "        self.mb = 1e-8\n",
    "        self.vw = 1e-8\n",
    "        self.vb = 1e-8\n",
    "        self.vhatw = 1e-8\n",
    "        self.vhatb = 1e-8\n",
    "\n",
    "\n",
    "    def update(self, grad_w, grad_b, t=1, *args, **kwds):\n",
    "        self.mw += (1-self.beta1)*(grad_w - self.mw)\n",
    "        self.mb += (1-self.beta1)*(grad_b - self.mb)\n",
    "        self.vw += (1-self.beta2)*(grad_w**2 - self.vw)\n",
    "        self.vb += (1-self.beta2)*(grad_b**2 - self.vb)\n",
    "        self.vhatw = np.maximum(self.vhatw, self.vw)\n",
    "        self.vhatb = np.maximum(self.vhatb, self.vb)\n",
    "\n",
    "        etal = self.eta*(1 - 1/((1-self.beta2)*t + 1))\n",
    "        etau = self.eta*(1 + 1/((1-self.beta2)*t + self.eps))\n",
    "\n",
    "        etahatw_t = np.clip(self.alpha/np.sqrt(self.vhatw), etal, etau)\n",
    "        etahatb_t = np.clip(self.alpha/np.sqrt(self.vhatb), etal, etau)\n",
    "\n",
    "        etaw_t = etahatw_t/np.sqrt(t)\n",
    "        etab_t = etahatb_t/np.sqrt(t)\n",
    "\n",
    "        dw = - etaw_t*self.mw\n",
    "        db = - etab_t*self.mb\n",
    "\n",
    "        return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_opt_dic = {\n",
    "    \"SGD\": SGD,\n",
    "    \"MSGD\": MSGD,\n",
    "    \"NAG\": NAG,\n",
    "    \"AdaGrad\": AdaGrad,\n",
    "    \"RMSprop\": RMSprop,\n",
    "    \"AdaDelta\": AdaDelta,\n",
    "    \"Adam\": Adam,\n",
    "    \"RMSpropGraves\": RMSpropGraves,\n",
    "    \"SMORMS3\": SMORMS3,\n",
    "    \"AdaMax\": AdaMax,\n",
    "    \"Nadam\": Nadam,\n",
    "    \"Eve\": Eve,\n",
    "    \"SantaE\": SantaE,\n",
    "    \"SantaSSS\": SantaSSS,\n",
    "    \"AMSGrad\": AMSGrad,\n",
    "    \"AdaBound\": AdaBound,\n",
    "    \"AMSBound\": AMSBound,\n",
    "}\n",
    "\n",
    "\n",
    "def get_opt(name, *args, **kwds):\n",
    "    if name in _opt_dic.keys():\n",
    "        optimizer = _opt_dic[name](*args, **kwds)\n",
    "    else:\n",
    "        raise ValueError(name + \": Unknown optimizer\")\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN util\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def im2col(images, filters, stride=1, pad=0):\n",
    "    if images.ndim == 2:\n",
    "        images = images.reshape(1, 1, *images.shape)\n",
    "    elif images.ndim == 3:\n",
    "        B, I_h, I_w = images.shape\n",
    "        images = images.reshape(B, 1, I_h, I_w)\n",
    "    B, C, I_h, I_w = images.shape\n",
    "    if isinstance(filters, tuple):\n",
    "        if len(filters) == 2:\n",
    "            filters = (1, 1, *filters)\n",
    "        elif len(filters) == 3:\n",
    "            M, F_h, F_w = filters\n",
    "            filters = (M, 1, F_h, F_w)\n",
    "        _, _, F_h, F_w = filters\n",
    "    else:\n",
    "        if filters.ndim == 2:\n",
    "            filters = filters.reshape(1, 1, *filters.shape)\n",
    "        elif filters.ndim == 3:\n",
    "            M, F_h, F_w = filters.shape\n",
    "            filters = filters.reshape(M, 1, F_h, F_w)\n",
    "        _, _, F_h, F_w = filters.shape\n",
    "    \n",
    "    if isinstance(stride, tuple):\n",
    "        stride_ud, stride_lr = stride\n",
    "    else:\n",
    "        stride_ud = stride\n",
    "        stride_lr = stride\n",
    "    if isinstance(pad, tuple):\n",
    "        pad_ud, pad_lr = pad\n",
    "    elif isinstance(pad, int):\n",
    "        pad_ud = pad\n",
    "        pad_lr = pad\n",
    "    elif pad == \"same\":\n",
    "        pad_ud = 0.5*((I_h - 1)*stride_ud - I_h + F_h)\n",
    "        pad_lr = 0.5*((I_w - 1)*stride_lr - I_w + F_w)\n",
    "    pad_zero = (0, 0)\n",
    "    \n",
    "    def get_O_shape(i, f, s, p):\n",
    "        return int((i - f + 2*p)//s + 1)\n",
    "    \n",
    "    O_h = get_O_shape(I_h, F_h, stride_ud, pad_ud)\n",
    "    O_w = get_O_shape(I_w, F_w, stride_lr, pad_lr)\n",
    "    \n",
    "    result_pad = (pad_ud, pad_lr)\n",
    "    pad_ud = int(np.ceil(pad_ud))\n",
    "    pad_lr = int(np.ceil(pad_lr))\n",
    "    pad_ud = (pad_ud, pad_ud)\n",
    "    pad_lr = (pad_lr, pad_lr)\n",
    "    images = np.pad(images, [pad_zero, pad_zero, pad_ud, pad_lr], \\\n",
    "                    \"constant\")\n",
    "    \n",
    "    cols = np.empty((B, C, F_h, F_w, O_h, O_w))\n",
    "    for h in range(F_h):\n",
    "        h_lim = h + stride_ud*O_h\n",
    "        for w in range(F_w):\n",
    "            w_lim = w + stride_lr*O_w\n",
    "            cols[:, :, h, w, :, :] \\\n",
    "                = images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr]\n",
    "    \n",
    "    results = []\n",
    "    results.append(cols.transpose(1, 2, 3, 0, 4, 5).reshape(C*F_h*F_w, B*O_h*O_w))\n",
    "    results.append((O_h, O_w))\n",
    "    results.append(result_pad)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def col2im(cols, I_shape, O_shape, stride=1, pad=0):\n",
    "    def get_f_shape(i, o, s, p):\n",
    "        return int(i + 2*p - (o - 1)*s)\n",
    "    \n",
    "    if len(I_shape) == 2:\n",
    "        B, C, I_h, I_w = 1, 1, *I_shape\n",
    "    elif len(I_shape) == 3:\n",
    "        C, B, I_h, I_w = 1, *I_shape\n",
    "    else:\n",
    "        B, C, I_h, I_w = I_shape\n",
    "    M, O_h, O_w = O_shape\n",
    "    \n",
    "    if isinstance(stride, tuple):\n",
    "        stride_ud, stride_lr = stride\n",
    "    else:\n",
    "        stride_ud = stride\n",
    "        stride_lr = stride\n",
    "    if isinstance(pad, tuple):\n",
    "        pad_ud, pad_lr = pad\n",
    "    elif isinstance(pad, int):\n",
    "        pad_ud = pad\n",
    "        pad_lr = pad\n",
    "    \n",
    "    F_h = get_f_shape(I_h, O_h, stride_ud, pad_ud)\n",
    "    F_w = get_f_shape(I_w, O_w, stride_lr, pad_lr)\n",
    "    pad_ud = int(np.ceil(pad_ud))\n",
    "    pad_lr = int(np.ceil(pad_lr))\n",
    "    cols = cols.reshape(C, F_h, F_w, B, O_h, O_w).transpose(3, 0, 1, 2, 4, 5)\n",
    "    images = np.zeros((B, C, I_h+2*pad_ud+stride_ud-1, I_w+2*pad_lr+stride_lr-1))\n",
    "    \n",
    "    for h in range(F_h):\n",
    "        h_lim = h + stride_ud*O_h\n",
    "        for w in range(F_w):\n",
    "            w_lim = w + stride_lr*O_w\n",
    "            images[:, :, h:h_lim:stride_ud, w:w_lim:stride_lr] += cols[:, :, h, w, :, :]\n",
    "    \n",
    "    return images[:, :, pad_ud : I_h+pad_ud, pad_lr : I_w+pad_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レイヤ\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseLayer():\n",
    "    \"\"\"\n",
    "    全ての元となるレイヤークラス\n",
    "    中間層と出力層で共通する処理を記述する。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, prev=1, n=1,\n",
    "                 name=\"\", wb_width=5e-2,\n",
    "                 act=\"ReLU\", opt=\"Adam\",\n",
    "                 act_dic=None, opt_dic=None, **kwds):\n",
    "        if act_dic is None:\n",
    "            act_dic = {}\n",
    "        if opt_dic is None:\n",
    "            opt_dic = {}\n",
    "        \n",
    "        self.prev = prev  # 一つ前の層の出力数 = この層への入力数\n",
    "        self.n = n        # この層の出力数 = 次の層への入力数\n",
    "        self.name = name  # この層の名前\n",
    "\n",
    "        # 重みとバイアスを設定\n",
    "        self.w = wb_width*np.random.randn(prev, n)\n",
    "        self.b = wb_width*np.random.randn(n)\n",
    "\n",
    "        # 活性化関数(クラス)を取得\n",
    "        self.act = get_act(act, **act_dic)\n",
    "\n",
    "        # 最適化子(クラス)を取得\n",
    "        self.opt = get_opt(opt, **opt_dic)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順伝播の実装\n",
    "        \"\"\"\n",
    "        # 入力を記憶しておく\n",
    "        self.x = x.copy()\n",
    "\n",
    "        # 順伝播\n",
    "        self.u = x@self.w + self.b\n",
    "        self.y = self.act.forward(self.u)\n",
    "        \n",
    "        return self.y\n",
    "\n",
    "\n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        逆伝播の実装\n",
    "        \"\"\"\n",
    "        dact = grad*self.act.backward(self.u, self.y)\n",
    "        self.grad_w = self.x.T@dact\n",
    "        self.grad_b = np.sum(dact, axis=0)\n",
    "        self.grad_x = dact@self.w.T\n",
    "\n",
    "        return self.grad_x\n",
    "\n",
    "\n",
    "    def update(self, **kwds):\n",
    "        \"\"\"\n",
    "        パラメータ学習の実装\n",
    "        \"\"\"\n",
    "        dw, db = self.opt.update(self.grad_w, self.grad_b, **kwds)\n",
    "\n",
    "        self.w += dw\n",
    "        self.b += db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MiddleLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    中間層クラス\n",
    "    入力層も実装上は中間層の一つとして取り扱います。\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "softmax = type(get_act(\"softmax\"))\n",
    "sigmoid = type(get_act(\"sigmoid\"))\n",
    "cross = type(get_err(\"Cross\"))\n",
    "binary = type(get_err(\"Binary\"))\n",
    "\n",
    "\n",
    "class OutputLayer(BaseLayer):\n",
    "    \"\"\"\n",
    "    出力層クラス\n",
    "    \"\"\"\n",
    "    def __init__(self, *, err_func=\"Square\", **kwds):\n",
    "        # 損失関数(クラス)を取得\n",
    "        self.errfunc = get_err(err_func)\n",
    "\n",
    "        super().__init__(**kwds)\n",
    "    \n",
    "\n",
    "    def backward(self, t):\n",
    "        \"\"\"\n",
    "        逆伝播の実装\n",
    "        \"\"\"\n",
    "        # 出力層の活性化関数がsoftmax関数で損失関数が交差エントロピー誤差の場合\n",
    "        # 誤差の伝播を場合分けしておく\n",
    "        if isinstance(self.act, softmax) \\\n",
    "        and isinstance(self.errfunc, cross):\n",
    "            dact = self.y - t\n",
    "            self.grad_w = self.x.T@dact\n",
    "            self.grad_b = np.sum(dact, axis=0)\n",
    "            self.grad_x = dact@self.w.T\n",
    "\n",
    "            return self.grad_x\n",
    "        elif isinstance(self.act, sigmoid) \\\n",
    "         and isinstance(self.errfunc, binary):\n",
    "            dact = self.y - t\n",
    "            self.grad_w = self.x.T@dact\n",
    "            self.grad_b = np.sum(dact, axis=0)\n",
    "            self.grad_x = dact@self.w.T\n",
    "\n",
    "            return self.grad_x\n",
    "        else:\n",
    "            grad = self.errfunc.backward(self.y, t)\n",
    "            return super().backward(grad)\n",
    "\n",
    "\n",
    "    def get_error(self, t):\n",
    "        self.error = self.errfunc.forward(self.y, t) / t.shape[0]\n",
    "        return self.errfunc.total_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ConvLayer(BaseLayer):\n",
    "    def __init__(self, *, I_shape=None, F_shape=None,\n",
    "                 stride=1, pad=\"same\",\n",
    "                 name=\"\", wb_width=5e-2,\n",
    "                 act=\"ReLU\", opt=\"Adam\",\n",
    "                 act_dic={}, opt_dic={}, **kwds):\n",
    "        self.name = name\n",
    "        \n",
    "        if I_shape is None:\n",
    "            raise KeyError(\"Input shape is None.\")\n",
    "        if F_shape is None:\n",
    "            raise KeyError(\"Filter shape is None.\")\n",
    "        \n",
    "        if len(I_shape) == 2:\n",
    "            C, I_h, I_w = 1, *I_shape\n",
    "        else:\n",
    "            C, I_h, I_w = I_shape\n",
    "        self.I_shape = (C, I_h, I_w)\n",
    "        \n",
    "        if len(F_shape) == 2:\n",
    "            M, F_h, F_w = 1, *F_shape\n",
    "        else:\n",
    "            M, F_h, F_w = F_shape\n",
    "        self.F_shape = (M, C, F_h, F_w)\n",
    "        \n",
    "        _, O_shape, self.pad_state = im2col(np.zeros((1, *self.I_shape)), self.F_shape,\n",
    "                                            stride=stride, pad=pad)\n",
    "        self.O_shape = (M, *O_shape)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.n = np.prod(self.O_shape)\n",
    "        \n",
    "        # フィルタとバイアスを設定\n",
    "        self.w = wb_width*np.random.randn(*self.F_shape).reshape(M, -1).T\n",
    "        self.b = wb_width*np.random.randn(M)\n",
    "        \n",
    "        # 活性化関数(クラス)を取得\n",
    "        self.act = get_act(act, **act_dic)\n",
    "\n",
    "        # 最適化子(クラス)を取得\n",
    "        self.opt = get_opt(opt, **opt_dic)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        M, O_h, O_w = self.O_shape\n",
    "        \n",
    "        x, _, self.pad_state = im2col(x, self.F_shape,\n",
    "                                      stride=self.stride,\n",
    "                                      pad=self.pad_state)\n",
    "        super().forward(x.T)\n",
    "        return self.y.reshape(B, O_h, O_w, M).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    \n",
    "    def backward(self, grad):\n",
    "        B = grad.shape[0]\n",
    "        I_shape = B, *self.I_shape\n",
    "        M, O_h, O_w = self.O_shape\n",
    "        \n",
    "        grad = grad.transpose(0, 2, 3, 1).reshape(-1, M)\n",
    "        super().backward(grad)\n",
    "        self.grad_x = col2im(self.grad_x.T, I_shape, self.O_shape,\n",
    "                             stride=self.stride, pad=self.pad_state)\n",
    "        return self.grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PoolingLayer(BaseLayer):\n",
    "    def __init__(self, *, I_shape=None,\n",
    "                 pool=1, pad=0,\n",
    "                 name=\"\", **kwds):\n",
    "        self.name = name\n",
    "        \n",
    "        if I_shape is None:\n",
    "            raise KeyError(\"Input shape is None.\")\n",
    "        \n",
    "        if len(I_shape) == 2:\n",
    "            C, I_h, I_w = 1, *I_shape\n",
    "        else:\n",
    "            C, I_h, I_w = I_shape\n",
    "        self.I_shape = (C, I_h, I_w)\n",
    "        \n",
    "        _, O_shape, self.pad_state = im2col(np.zeros((1, *self.I_shape)), (pool, pool),\n",
    "                                            stride=pool, pad=pad)\n",
    "        self.O_shape = (C, *O_shape)\n",
    "        \n",
    "        self.n = np.prod(self.O_shape)\n",
    "        \n",
    "        self.pool = pool\n",
    "        self.F_shape = (pool, pool)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        C, O_h, O_w = self.O_shape\n",
    "        \n",
    "        self.x, _, self.pad_state = im2col(x, self.F_shape,\n",
    "                                           stride=self.pool,\n",
    "                                           pad=self.pad_state)\n",
    "        \n",
    "        self.x = self.x.T.reshape(B*O_h*O_w*C, -1)\n",
    "        self.max_index = np.argmax(self.x, axis=1)\n",
    "        self.y = np.max(self.x, axis=1).reshape(B, O_h, O_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    def backward(self, grad):\n",
    "        B = grad.shape[0]\n",
    "        I_shape = B, *self.I_shape\n",
    "        C, O_h, O_w = self.O_shape\n",
    "        \n",
    "        grad = grad.transpose(0, 2, 3, 1).reshape(-1, 1)\n",
    "        self.grad_x = np.zeros((grad.size, self.pool*self.pool))\n",
    "        self.grad_x[:, self.max_index] = grad\n",
    "        self.grad_x = self.grad_x.reshape(B*O_h*O_w, C*self.pool*self.pool).T\n",
    "        self.grad_x = col2im(self.grad_x, I_shape, self.O_shape,\n",
    "                             stride=self.pool, pad=self.pad_state)\n",
    "        \n",
    "        return self.grad_x\n",
    "    \n",
    "    \n",
    "    def update(self, **kwds):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レイヤマネージャ\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerManagerError(Exception):\n",
    "    \"\"\"レイヤーモジュールにおけるユーザ定義エラーのベースクラス\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AssignError(LayerManagerError):\n",
    "    def __init__(self, value=None):\n",
    "        if not value is None:\n",
    "            self.value = value\n",
    "            self.message = (str(value)\n",
    "                         + \": Assigning that value is prohibited.\")\n",
    "        else:\n",
    "            self.value = None\n",
    "            self.message = \"Assigning that value is prohibited.\"\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "class UnmatchUnitError(LayerManagerError):\n",
    "    def __init__(self, prev, n):\n",
    "        self.prev = prev\n",
    "        self.n = n\n",
    "\n",
    "        self.message = \"Unmatch units: <{}> and <{}>.\".format(prev, n)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message\n",
    "\n",
    "\n",
    "class UndefinedLayerError(LayerManagerError):\n",
    "    def __init__(self, type_name):\n",
    "        self.type = type_name\n",
    "        self.message = \"<{}>: Undefined layer type.\".format(str(type_name))\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TypeManager():\n",
    "    \"\"\"\n",
    "    層の種類に関するマネージャクラス\n",
    "    \"\"\"\n",
    "    N_TYPE = 4  # 層の種類数\n",
    "\n",
    "    BASE = -1\n",
    "    MIDDLE = 0  # 中間層のナンバリング\n",
    "    OUTPUT = 1  # 出力層のナンバリング\n",
    "    CONV = 2    #畳み込み層のナンバリング\n",
    "    POOL = 3    #プーリング層のナンバリング\n",
    "    \n",
    "    REGULATED_DIC = {\"Middle\": MiddleLayer,\n",
    "                     \"Output\": OutputLayer,\n",
    "                     \"Conv\": ConvLayer,\n",
    "                     \"Pool\": PoolingLayer,\n",
    "                     \"BaseLayer\": None}\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def reg_keys(self):\n",
    "        return list(self.REGULATED_DIC.keys())\n",
    "    \n",
    "    \n",
    "    def name_rule(self, name):\n",
    "        name = name.lower()\n",
    "        if \"middle\" in name or name == \"mid\" or name == \"m\":\n",
    "            name = self.reg_keys[self.MIDDLE]\n",
    "        elif \"output\" in name or name == \"out\" or name == \"o\":\n",
    "            name = self.reg_keys[self.OUTPUT]\n",
    "        elif \"conv\" in name or name == \"c\":\n",
    "            name = self.reg_keys[self.CONV]\n",
    "        elif \"pool\" in name or name == \"p\":\n",
    "            name = self.reg_keys[self.POOL]\n",
    "        else:\n",
    "            raise UndefinedLayerError(name)\n",
    "        \n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = type(get_act(\"softmax\"))\n",
    "sigmoid = type(get_act(\"sigmoid\"))\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, x, y):\n",
    "        self.x_train, self.x_test = x\n",
    "        self.y_train, self.y_test = y\n",
    "        \n",
    "        self.make_anim = False\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        n_batch = x.shape[0]\n",
    "        switch = True\n",
    "        for ll in self.layer_list:\n",
    "            if switch and not self.is_CNN(ll.name):\n",
    "                x_in = x_in.reshape(n_batch, -1)\n",
    "                switch = False\n",
    "            x_in = ll.forward(x_in)\n",
    "    \n",
    "    \n",
    "    def backward(self, t):\n",
    "        y_in = t\n",
    "        n_batch = t.shape[0]\n",
    "        switch = True\n",
    "        for ll in self.layer_list[::-1]:\n",
    "            if switch and self.is_CNN(ll.name):\n",
    "                y_in = y_in.reshape(n_batch, *ll.O_shape)\n",
    "                switch = False\n",
    "            y_in = ll.backward(y_in)\n",
    "    \n",
    "    \n",
    "    def update(self, **kwds):\n",
    "        for ll in self.layer_list:\n",
    "            ll.update(**kwds)\n",
    "    \n",
    "    \n",
    "    def training(self, epoch, n_batch=16, threshold=1e-8,\n",
    "                 show_error=True, show_train_error=False, **kwds):\n",
    "        if show_error:\n",
    "            self.error_list = []\n",
    "        if show_train_error:\n",
    "            self.train_error_list = []\n",
    "        if self.make_anim:\n",
    "            self.images = []\n",
    "        \n",
    "        n_train = self.x_train.shape[0]//n_batch\n",
    "        n_test = self.x_test.shape[0]\n",
    "        \n",
    "        # 学習開始\n",
    "        error = 0\n",
    "        error_prev = 0\n",
    "        rand_index = np.arange(self.x_train.shape[0])\n",
    "        for t in tqdm.tqdm(range(1, epoch+1)):\n",
    "            #シーン作成\n",
    "            if self.make_anim:\n",
    "                self.make_scene(t, epoch)\n",
    "            \n",
    "            # 訓練誤差計算\n",
    "            if show_train_error:\n",
    "                self.forward(self.x_train)\n",
    "                error = lm[-1].get_error(self.y_train)\n",
    "                self.train_error_list.append(error)\n",
    "            \n",
    "            # 誤差計算\n",
    "            self.forward(self.x_test)\n",
    "            error = lm[-1].get_error(self.y_test)\n",
    "            if show_error:\n",
    "                self.error_list.append(error)\n",
    "\n",
    "            # 収束判定\n",
    "            if np.isnan(error):\n",
    "                print(\"fail training...\")\n",
    "                break\n",
    "            if abs(error - error_prev) < threshold:\n",
    "                print(\"end learning...\")\n",
    "                break\n",
    "            else:\n",
    "                error_prev = error\n",
    "\n",
    "            np.random.shuffle(rand_index)\n",
    "            for i in range(n_train):\n",
    "                rand = rand_index[i*n_batch : (i+1)*n_batch]\n",
    "                \n",
    "                self.forward(self.x_train[rand])\n",
    "                self.backward(self.y_train[rand])\n",
    "                self.update(**kwds)\n",
    "        \n",
    "        if show_error:\n",
    "            # 誤差遷移表示\n",
    "            self.show_errors(show_train_error, **kwds)\n",
    "    \n",
    "    \n",
    "    def pred_func(self, y, threshold=0.5):\n",
    "        if isinstance(self[-1].act, softmax):\n",
    "            return np.argmax(y, axis=1)\n",
    "        elif isinstance(self[-1].act, sigmoid):\n",
    "            return np.where(y > threshold, 1, 0)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "    \n",
    "    \n",
    "    def predict(self, x=None, y=None, threshold=0.5):\n",
    "        if x is None:\n",
    "            x = self.x_test\n",
    "        if y is None:\n",
    "            y = self.y_test\n",
    "        \n",
    "        self.forward(x)\n",
    "        self.y_pred = self.pred_func(self[-1].y, threshold)\n",
    "        y = self.pred_func(y, threshold)\n",
    "        print(y[:16], self.y_pred[:16])\n",
    "        print(\"accuracy rate:\", np.sum(self.y_pred == y, dtype=int)/y.shape[0]*100, \"%\",\n",
    "              \"({}/{})\".format(np.sum(self.y_pred == y, dtype=int), y.shape[0]))\n",
    "    \n",
    "    \n",
    "    def show_errors(self, show_train_error=False, title=\"error transition\",\n",
    "                    xlabel=\"epoch\", ylabel=\"error\", fname=\"error_transition.png\",\n",
    "                    log_scale=True, **kwds):\n",
    "        fig, ax = plt.subplots(1)\n",
    "        fig.suptitle(title)\n",
    "        if log_scale:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.grid()\n",
    "        if show_train_error:\n",
    "            ax.plot(self.train_error_list, label=\"train accuracy\")\n",
    "        ax.plot(self.error_list, label=\"test accuracy\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        #fig.show()\n",
    "        if len(fname) != 0:\n",
    "            fig.savefig(fname)\n",
    "    \n",
    "    \n",
    "    def ready_anim(self, n_image, x, y, title=\"animation\",\n",
    "                   xlabel=\"x\", ylabel=\"y\", ex_color=\"r\", color=\"b\",\n",
    "                   x_left=0, x_right=0, y_down = 1, y_up = 1):\n",
    "        self.n_image = n_image\n",
    "        self.x = x\n",
    "        self.color = color\n",
    "        self.make_anim = True\n",
    "        \n",
    "        self.anim_fig, self.anim_ax = plt.subplots(1)\n",
    "        self.anim_fig.suptitle(title)\n",
    "        self.anim_ax.set_xlabel(xlabel)\n",
    "        self.anim_ax.set_ylabel(ylabel)\n",
    "        self.anim_ax.set_xlim(np.min(x) - x_left, np.max(x) + x_right)\n",
    "        self.anim_ax.set_ylim(np.min(y) - y_down, np.max(y) + y_up)\n",
    "        self.anim_ax.grid()\n",
    "        self.anim_ax.plot(x, y, color=ex_color)\n",
    "        \n",
    "        return self.anim_fig, self.anim_ax\n",
    "    \n",
    "    \n",
    "    def make_scene(self, t, epoch):\n",
    "        # シーン作成\n",
    "        if t % (epoch/self.n_image) == 1:\n",
    "            x_in = self.x.reshape(-1, 1)\n",
    "            for ll in self.layer_list:\n",
    "                x_in = ll.forward(x_in)\n",
    "            im, = self.anim_ax.plot(self.x, ll.y, color=self.color)\n",
    "            self.images.append([im])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class LayerManager(_TypeManager, Trainer):\n",
    "    \"\"\"\n",
    "    層を管理するためのマネージャクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__(x, y)\n",
    "        \n",
    "        self.__layer_list = []  # レイヤーのリスト\n",
    "        self.__name_list = []   # 各レイヤーの名前リスト\n",
    "        self.__ntype = np.zeros(self.N_TYPE, dtype=int)  # 種類別レイヤーの数\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        layerRepr= \"layer_list: \" + repr(self.__layer_list)\n",
    "        nameRepr = \"name_list: \" + repr(self.__name_list)\n",
    "        ntypeRepr = \"ntype: \" + repr(self.__ntype)\n",
    "        return (layerRepr + \"\\n\"\n",
    "                + nameRepr + \"\\n\"\n",
    "                + ntypeRepr)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        layerStr = \"layer_list: \" + str(self.__layer_list)\n",
    "        nameStr = \"name_list: \" + str(self.__name_list)\n",
    "        ntypeStr = \"ntype: \" + str(self.__ntype)\n",
    "        return (layerStr + \"\\n\"\n",
    "                + nameStr + \"\\n\"\n",
    "                + ntypeStr)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Pythonのビルドイン関数`len`から呼ばれたときの動作を記述。\n",
    "        種類別レイヤーの数の総和を返します。\n",
    "        \"\"\"\n",
    "        return int(np.sum(self.__ntype))\n",
    "\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        例えば\n",
    "        lm = LayerManager()\n",
    "\n",
    "        +----------------+\n",
    "        | (lmに要素を追加) |\n",
    "        +----------------+\n",
    "\n",
    "        x = lm[3].~~\n",
    "        のように、リストや配列の要素にアクセスされたときに呼ばれるので、\n",
    "        そのときの動作を記述。\n",
    "        sliceやstr, intでのアクセスのみ許可します。\n",
    "        \"\"\"\n",
    "        if isinstance(key, slice):\n",
    "            # keyがスライスならレイヤーのリストをsliceで参照する。\n",
    "            # 異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            return self.__layer_list[key]\n",
    "        elif isinstance(key, str):\n",
    "            # keyが文字列なら各レイヤーの名前リストからインデックスを取得して、\n",
    "            # 該当するレイヤーのリストの要素を返す。\n",
    "            if key in self.__name_list:\n",
    "                index = self.__name_list.index(key)\n",
    "                return self.__layer_list[index]\n",
    "            else:\n",
    "                # keyが存在しない場合はKeyErrorを出す。\n",
    "                raise KeyError(\"{}: No such item\".format(key))\n",
    "        elif isinstance(key, int):\n",
    "            # keyが整数ならレイヤーのリストの該当要素を返す。\n",
    "            # 異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            return self.__layer_list[key]\n",
    "        else:\n",
    "            raise KeyError(key, \": Undefined such key type.\")\n",
    "\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        例えば\n",
    "        lm = LayerManager()\n",
    "\n",
    "        +----------------+\n",
    "        | (lmに要素を追加) |\n",
    "        +----------------+\n",
    "\n",
    "        lm[1] = x\n",
    "        のように、リストや配列の要素にアクセスされたときに呼ばれるので、\n",
    "        そのときの動作を記述。\n",
    "        要素の上書きのみ認め、新規要素の追加などは禁止します。\n",
    "        \"\"\"\n",
    "        value_type = \"\"\n",
    "        if isinstance(value, list):\n",
    "            # 右辺で指定された'value'が'list'なら\n",
    "            # 全ての要素が'BaseLayer'クラスかそれを継承していなければエラー。\n",
    "            if not np.all(\n",
    "                np.where(isinstance(value, BaseLayer), True, False)):\n",
    "                self.AssignError()\n",
    "            value_type = \"list\"\n",
    "        elif isinstance(value, BaseLayer):\n",
    "            # 右辺で指定された'value'が'BaseLayer'クラスか\n",
    "            # それを継承していない場合はエラー。\n",
    "            self.AssignError(type(value))\n",
    "        if value_type == \"\":\n",
    "            value_type = self.reg_keys[self.BASE]\n",
    "\n",
    "        if isinstance(key, slice):\n",
    "            # keyがスライスならレイヤーのリストの要素を上書きする。\n",
    "            # ただし'value_type'が'list'でなければエラー。\n",
    "            # 異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            if value_type != \"list\":\n",
    "                self.AssignError(value_type)\n",
    "            self.__layer_list[key] = value\n",
    "        elif isinstance(key, str):\n",
    "            # keyが文字列なら各レイヤーの名前リストからインデックスを取得して、\n",
    "            # 該当するレイヤーのリストの要素を上書きする。\n",
    "            # ただし'value_type'が'BaseLayer'でなければエラー。\n",
    "            if value_type != self.reg_keys[self.BASE]:\n",
    "                raise AssignError(value_type)\n",
    "            if key in self.__name_list:\n",
    "                index = self.__name_list.index(key)\n",
    "                self.__layer_list[index] = value\n",
    "            else:\n",
    "                # keyが存在しない場合はKeyErrorを出す。\n",
    "                raise KeyError(\"{}: No such item\".format(key))\n",
    "        elif isinstance(key, int):\n",
    "            # keyが整数ならレイヤーのリストの該当要素を上書きする。\n",
    "            # ただし'value_type'が'BaseLayer'でなければエラー。\n",
    "            # また、異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            if value_type != self.reg_keys[self.BASE]:\n",
    "                raise AssignError(value_type)\n",
    "            self.__layer_list[key] = value\n",
    "        else:\n",
    "            raise KeyError(key, \": Undefined such key type.\")\n",
    "\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        \"\"\"\n",
    "        例えば\n",
    "        lm = LayerManager()\n",
    "\n",
    "        +----------------+\n",
    "        | (lmに要素を追加) |\n",
    "        +----------------+\n",
    "\n",
    "        del lm[2]\n",
    "        のように、del文でリストや配列の要素にアクセスされたときに呼ばれるので、\n",
    "        そのときの動作を記述。\n",
    "        指定要素が存在すれば削除、さらにリネームを行います。\n",
    "        \"\"\"\n",
    "        if isinstance(key, slice):\n",
    "            # keyがスライスならそのまま指定の要素を削除\n",
    "            # 異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            del self.__layer_list[slice]\n",
    "            del self.__name_list[slice]\n",
    "        elif isinstance(key, str):\n",
    "            # keyが文字列なら各レイヤーの名前リストからインデックスを取得して、\n",
    "            # 該当する要素を削除する。\n",
    "            if key in self.__name_list:\n",
    "                del self.__layer_list[index]\n",
    "                del self.__name_list[index]\n",
    "            else:\n",
    "                # keyが存在しない場合はKeyErrorを出す。\n",
    "                raise KeyError(\"{}: No such item\".format(key))\n",
    "        elif isinstance(key, int):\n",
    "            # keyが整数ならレイヤーのリストの該当要素を削除する。\n",
    "            # 異常な値(Index out of rangeなど)が入力されたら\n",
    "            # Pythonがエラーを出してくれます。\n",
    "            del self.__layer_list[key]\n",
    "        else:\n",
    "            raise KeyError(key, \": Undefined such key type.\")\n",
    "\n",
    "        # リネームする\n",
    "        self._rename()\n",
    "\n",
    "\n",
    "    def _rename(self):\n",
    "        \"\"\"\n",
    "        リスト操作によってネームリストのネーミングがルールに反するものになった場合に\n",
    "        改めてルールを満たすようにネーミングリストおよび各レイヤーの名前を変更する。\n",
    "\n",
    "        ネーミングルールは[レイヤーの種類][何番目か]とします。\n",
    "        レイヤーの種類はMiddleLayerならMiddle\n",
    "                     OutputLayerならOutput\n",
    "        のように略します。\n",
    "        何番目かというのは種類別でカウントします。\n",
    "\n",
    "        また、ここで改めて__ntypeのカウントを行います。\n",
    "        \"\"\"\n",
    "        # 種類別レイヤーの数を初期化\n",
    "        self.__ntype = np.zeros(self.N_TYPE)\n",
    "\n",
    "        # 再カウントと各レイヤーのリネーム\n",
    "        for i in range(len(self)):\n",
    "            for j, reg_name in enumerate(self.REGULATED_DIC):\n",
    "                if reg_name in self.__name_list[i]:\n",
    "                    self.__ntype[j] += 1\n",
    "                    self.__name_list[i] = (self.reg_keys[j]\n",
    "                                        + str(self.__ntype[j]))\n",
    "                    self.__layer_list[i].name = (self.reg_keys[j]\n",
    "                                              + str(self.__ntype[j]))\n",
    "                    break\n",
    "            else:\n",
    "                raise UndefinedLayerType(self.__name_list[i])\n",
    "    \n",
    "\n",
    "    def append(self, *, name=\"Middle\", **kwds):\n",
    "        \"\"\"\n",
    "        リストに要素を追加するメソッドでお馴染みのappendメソッドの実装。\n",
    "        \"\"\"\n",
    "        if \"prev\" in kwds:\n",
    "            # 'prev'がキーワードに含まれている場合、\n",
    "            # 一つ前の層の要素数を指定していることになります。\n",
    "            # 基本的に最初のレイヤーを挿入する時を想定していますので、\n",
    "            # それ以外は基本的に自動で決定するため指定しません。\n",
    "            if len(self) != 0:\n",
    "                if kwds[\"prev\"] != self.__layer_list[-1].n:\n",
    "                    # 最後尾のユニット数と一致しなければエラー。\n",
    "                    raise UnmatchUnitError(self.__layer_list[-1].n,\n",
    "                                           kwds[\"prev\"])\n",
    "        elif not self.is_CNN(name):\n",
    "            if len(self) == 0:\n",
    "                # 最初のDNNレイヤは必ず入力ユニットの数を指定する必要があります。\n",
    "                raise UnmatchUnitError(\"Input units\", \"Unspecified\")\n",
    "            else:\n",
    "                # 最後尾のレイヤのユニット数を'kwds'に追加\n",
    "                kwds[\"prev\"] = self.__layer_list[-1].n\n",
    "\n",
    "        # レイヤーの種類を読み取り、ネーミングルールに則った名前に変更する\n",
    "        name = self.name_rule(name)\n",
    "\n",
    "        # レイヤーを追加する。\n",
    "        for i, reg_name in enumerate(self.REGULATED_DIC):\n",
    "            if name in reg_name:\n",
    "                # 種類別レイヤーをインクリメントして\n",
    "                self.__ntype[i] += 1\n",
    "                # 名前に追加し\n",
    "                name += str(self.__ntype[i])\n",
    "                # ネームリストに追加し\n",
    "                self.__name_list.append(name)\n",
    "                # 最後にレイヤーを生成してリストに追加します。\n",
    "                self.__layer_list.append(self.REGULATED_DIC[reg_name](name=name, **kwds))\n",
    "\n",
    "\n",
    "    def extend(self, lm):\n",
    "        \"\"\"\n",
    "        extendメソッドでは既にある別のレイヤーマネージャ'lm'の要素を\n",
    "        全て追加します。\n",
    "        \"\"\"\n",
    "        if not isinstance(lm, LayerManager):\n",
    "            # 'lm'のインスタンスがLayerManagerでなければエラー。\n",
    "            raise TypeError(type(lm), \": Unexpected type.\")\n",
    "        if len(self) != 0:\n",
    "            if self.__layer_list[-1].n != lm[0].prev:\n",
    "                # 自分の最後尾のレイヤーのユニット数と\n",
    "                # 'lm'の最初のレイヤーの入力数が一致しない場合はエラー。\n",
    "                raise UnmatchUnitError(self.__layer_list[-1].n,\n",
    "                                       lm[0].prev)\n",
    "\n",
    "        # それぞれ'extend'メソッドで追加\n",
    "        self.__layer_list.extend(lm.layer_list)\n",
    "        self.__name_list.extend(lm.name_list)\n",
    "\n",
    "        # リネームする\n",
    "        self._rename()\n",
    "\n",
    "\n",
    "    def insert(self, prev_name, name=\"Middle\", **kwds):\n",
    "        \"\"\"\n",
    "        insertメソッドでは、前のレイヤーの名前を指定しそのレイヤーと結合するように\n",
    "        要素を追加します。\n",
    "        \"\"\"\n",
    "        # 'prev_name'が存在しなければエラー。\n",
    "        if not prev_name in self.__name_list:\n",
    "            raise KeyError(prev_name, \": No such key.\")\n",
    "        # 'prev'がキーワードに含まれている場合、\n",
    "        # 'prev_name'で指定されているレイヤーのユニット数と一致しなければエラー。\n",
    "        if \"prev\" in kwds:\n",
    "            if kwds[\"prev\"] \\\n",
    "                != self.__layer_list[self.index(prev_name)].n:\n",
    "                raise UnmatchUnitError(\n",
    "                    kwds[\"prev\"],\n",
    "                    self.__layer_list[self.index(prev_name)].n)\n",
    "        # 'n'がキーワードに含まれている場合、\n",
    "        if \"n\" in kwds:\n",
    "            # 'prev_name'が最後尾ではない場合は\n",
    "            if prev_name != self.__name_list[-1]:\n",
    "                # 次のレイヤーのユニット数と一致しなければエラー。\n",
    "                if kwds[\"n\"] != self.__layer_list[\n",
    "                        self.index(prev_name)+1].prev:\n",
    "                    raise UnmatchUnitError(\n",
    "                        kwds[\"n\"],\n",
    "                        self.__layer_list[self.index(prev_name)].prev)\n",
    "        # まだ何も要素がない場合は'append'メソッドを用いるようにエラーを出す。\n",
    "        if len(self) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"You have to use 'append' method instead.\")\n",
    "\n",
    "        # 挿入場所のインデックスを取得\n",
    "        index = self.index(prev_name) + 1\n",
    "\n",
    "        # レイヤーの種類を読み取り、ネーミングルールに則った名前に変更する\n",
    "        name = self.name_rule(name)\n",
    "\n",
    "        # 要素を挿入する\n",
    "        for i, reg_name in enumerate(self.REGULATED_DIC):\n",
    "            if reg_name in name:\n",
    "                self.__layer_list.insert(index,\n",
    "                                         self.REGULATED_DIC[reg_name](name=name, **kwds))\n",
    "                self.__name_list.insert(index,\n",
    "                                        self.REGULATED_DIC[reg_name](name=name, **kwds))\n",
    "\n",
    "        # リネームする\n",
    "        self._rename()\n",
    "\n",
    "\n",
    "    def extend_insert(self, prev_name, lm):\n",
    "        \"\"\"\n",
    "        こちらはオリジナル関数です。\n",
    "        extendメソッドとinsertメソッドを組み合わせたような動作をします。\n",
    "        簡単に説明すると、別のレイヤーマネージャをinsertする感じです。\n",
    "        \"\"\"\n",
    "        if not isinstance(lm, LayerManager):\n",
    "            # 'lm'のインスタンスがLayerManagerでなければエラー。\n",
    "            raise TypeError(type(lm), \": Unexpected type.\")\n",
    "        # 'prev_name'が存在しなければエラー。\n",
    "        if not prev_name in self.__name_list:\n",
    "            raise KeyError(prev_name, \": No such key.\")\n",
    "        # 指定場所の前後のレイヤーとlmの最初・最後のレイヤーのユニット数が\n",
    "        # それぞれ一致しなければエラー。\n",
    "        if len(self) != 0:\n",
    "            if self.__layer_list[self.index(prev_name)].n \\\n",
    "                    != lm.layer_list[0].prev:\n",
    "                # 自分の指定場所のユニット数と'lm'の最初のユニット数が\n",
    "                # 一致しなければエラー。\n",
    "                raise UnmatchUnitError(\n",
    "                    self.__layer_list[self.index(prev_name)].n,\n",
    "                    lm.layer_list[0].prev)\n",
    "            if prev_name != self.__name_list[-1]:\n",
    "                # 'prev_name'が自分の最後尾のレイヤーでなく\n",
    "                if lm.layer_list[-1].n \\\n",
    "                    != self.__layer_list[self.index(prev_name)+1].prev:\n",
    "                    # 'lm'の最後尾のユニット数と自分の指定場所の次のレイヤーの\n",
    "                    # 'prev'ユニット数と一致しなければエラー。\n",
    "                    raise UnmatchUnitError(\n",
    "                        lm.layer_list[-1].n,\n",
    "                        self.__layer_list[self.index(prev_name)+1].prev)\n",
    "        else:\n",
    "            # 自分に何の要素もない場合は'extend'メソッドを使うようにエラーを出す。\n",
    "            raise RuntimeError(\n",
    "                \"You have to use 'extend' method instead.\")\n",
    "\n",
    "        # 挿入場所のインデックスを取得\n",
    "        index = self.index(prev_name) + 1\n",
    "\n",
    "        # 挿入場所以降の要素を'buf'に避難させてから一旦取り除き、\n",
    "        # extendメソッドを使って要素を追加\n",
    "        layer_buf = self.__layer_list[index:]\n",
    "        name_buf = self.__name_list[index:]\n",
    "        del self.__layer_list[index:]\n",
    "        del self.__name_list[index:]\n",
    "        self.extend(lm)\n",
    "\n",
    "        # 避難させていた要素を追加する\n",
    "        self.__layer_list.extend(layer_buf)\n",
    "        self.__name_list.extend(name_buf)\n",
    "\n",
    "        # リネームする\n",
    "        self._rename()\n",
    "\n",
    "\n",
    "    def remove(self, key):\n",
    "        \"\"\"\n",
    "        removeメソッドでは指定の名前の要素を削除します。\n",
    "        インデックスでの指定も許可します。\n",
    "        \"\"\"\n",
    "        # 既に実装している'del'文でOKです。\n",
    "        del self[key]\n",
    "\n",
    "\n",
    "    def index(self, target):\n",
    "        return self.__name_list.index(target)\n",
    "\n",
    "\n",
    "    def name(self, indices):\n",
    "        return self.__name_list[indices]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def layer_list(self):\n",
    "        return self.__layer_list\n",
    "\n",
    "\n",
    "    @property\n",
    "    def name_list(self):\n",
    "        return self.__name_list\n",
    "\n",
    "\n",
    "    @property\n",
    "    def ntype(self):\n",
    "        return self.__ntype\n",
    "    \n",
    "    \n",
    "    def is_CNN(self, name=None):\n",
    "        if name is None:\n",
    "            if self.__ntype[self.CONV] > 0 \\\n",
    "            or self.__ntype[self.POOL] > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            name = self.name_rule(name)\n",
    "            if self.reg_keys[self.CONV] in name \\\n",
    "            or self.reg_keys[self.POOL] in name:\n",
    "                return True\n",
    "            else:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実験コード\n",
    "[目次へ戻る](#目次)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数近似編"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# 学習対象設定\n",
    "def split_test(target, train_indices):\n",
    "    return (target[train_indices], target[~ train_indices])\n",
    "x = np.arange(0, 4, 5e-2)\n",
    "y = np.sin(x)\n",
    "x_left = 1\n",
    "x_right = 3\n",
    "y_top = np.max(y) + 1\n",
    "y_bottom = np.min(y) - 1\n",
    "indices = (x_left <= x) & (x <= x_right)\n",
    "x_train, x_test = split_test(x, indices)\n",
    "y_train, y_test = split_test(y, indices)\n",
    "\n",
    "# 初期設定\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "epoch = 30000\n",
    "threshold = 1e-8\n",
    "n_batch = 8\n",
    "x_train = x_train.reshape(-1, n_in)\n",
    "x_test = x_test.reshape(-1, n_in)\n",
    "y_train = y_train.reshape(-1, n_in)\n",
    "y_test = y_test.reshape(-1, n_in)\n",
    "\n",
    "# ネットワーク構築\n",
    "lm = LayerManager((x_train, x_test), (y_train, y_test))\n",
    "lm.append(prev=n_in, n=30, act=\"tanhExp\")\n",
    "lm.append(n=30, act=\"tanhExp\")\n",
    "lm.append(n=n_out, name=\"o\", act=\"identity\")\n",
    "\n",
    "\n",
    "# アニメーションプロット用土台作成\n",
    "n_image = 100\n",
    "interval = 100\n",
    "fig, ax = lm.ready_anim(n_image, x, y, title=\"fitting animation\")\n",
    "ax.plot(np.full_like(np.arange(y_bottom, y_top+1), x_left),\n",
    "        np.arange(y_bottom, y_top+1),\n",
    "        color=\"g\")\n",
    "ax.plot(np.full_like(np.arange(y_bottom, y_top+1), x_right),\n",
    "        np.arange(y_bottom, y_top+1),\n",
    "        color=\"g\")\n",
    "\n",
    "# 学習開始\n",
    "lm.training(epoch, threshold=threshold, n_batch=n_batch)\n",
    "\n",
    "# フィッティングアニメーション作成\n",
    "anim = animation.ArtistAnimation(lm.anim_fig, lm.images,interval=interval, repeat_delay=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN実験編\n",
    "データ処理はいくつか種類がありますが、代表的な二つを紹介しておきます。\n",
    "\n",
    "[目次へ戻る](#目次)\n",
    "\n",
    "### 標準化\n",
    "標準化はデータを平均0、標準偏差1になるようにデータを縮尺する処理のことです。\n",
    "\\begin{align}\n",
    "  \\hat{x} = \\cfrac{x - \\mu}{\\sigma}\n",
    "\\end{align}\n",
    "ここでの$\\mu$は平均値、$\\sigma$は標準偏差です。\n",
    "\n",
    "### 正規化\n",
    "正規化はデータに何らかの処理を行いデータ値の大きさを0~1などに収める処理のことです。\n",
    "大抵は最大値と最小値を用いて\n",
    "\\begin{align}\n",
    "  \\hat{x} = \\cfrac{x - x_{min}}{x_{max} - x_{min}}\n",
    "\\end{align}\n",
    "とします。\n",
    "\n",
    "正規化は異常値の影響を大きく受けてしまうため一般には標準化を用いることが多いですが、画像データなどの異常値があり得ないデータの場合は正規化を行うことがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KerasのMNISTデータ処理\n",
    "使いたい方を実行してください。\n",
    "KerasのMNISTデータセットは実行時間が非常に大きくなるため注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# データセット取得\n",
    "n_class=10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "C, B, I_h, I_w = 1, *x_train.shape\n",
    "B_test = x_test.shape[0]\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train.reshape(B, -1)).reshape(B, C, I_h, I_w)\n",
    "x_test = sc.fit_transform(x_test.reshape(B_test, -1)).reshape(B_test, C, I_h, I_w)\n",
    "\n",
    "# one-hotラベルへの変換\n",
    "def to_one_hot(data, n_class):\n",
    "    vec = np.zeros((len(data), n_class))\n",
    "    for i in range(len(data)):\n",
    "        vec[i, data[i]] = 1.\n",
    "    return vec\n",
    "t_train = to_one_hot(y_train, n_class)\n",
    "t_test = to_one_hot(y_test, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learnのMNISTデータ処理\n",
    "scikit-learnの方はデータ量がすごく削減されているため実行時間はかなり短くて済みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# データセット取得\n",
    "n_class=10\n",
    "C, I_h, I_w = 1, 8, 8\n",
    "digits = datasets.load_digits()\n",
    "x = digits.data\n",
    "t = digits.target\n",
    "n_data = len(x)\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x).reshape(n_data, I_h, I_w)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.2, shuffle=True)\n",
    "\n",
    "# one-hotラベルへの変換\n",
    "def to_one_hot(data, n_class):\n",
    "    vec = np.zeros((len(data), n_class))\n",
    "    for i in range(len(data)):\n",
    "        vec[i, data[i]] = 1.\n",
    "    return vec\n",
    "t_train = to_one_hot(y_train, n_class)\n",
    "t_test = to_one_hot(y_test, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN実験コード本体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 畳み込み層と出力層を作成\n",
    "M, F_h, F_w = 10, 3, 3\n",
    "lm = LayerManager((x_train, x_test), (t_train, t_test))\n",
    "lm.append(name=\"c\", I_shape=(C, I_h, I_w), F_shape=(M, F_h, F_w), pad=1,\n",
    "          wb_width=0.1, opt=\"AdaDelta\", opt_dic={\"eta\": 1e-2})\n",
    "lm.append(name=\"p\", I_shape=lm[-1].O_shape, pool=2)\n",
    "lm.append(name=\"m\", n=100, wb_width=0.1,\n",
    "          opt=\"AdaDelta\", opt_dic={\"eta\": 1e-2})\n",
    "lm.append(name=\"o\", n=n_class, act=\"softmax\", err_func=\"Cross\", wb_width=0.1,\n",
    "          opt=\"AdaDelta\", opt_dic={\"eta\": 1e-2})\n",
    "\n",
    "# 学習させる\n",
    "epoch = 50\n",
    "threshold = 1e-8\n",
    "n_batch = 128\n",
    "lm.training(epoch, threshold=threshold, n_batch=n_batch, show_train_error=True)\n",
    "\n",
    "# 予測する\n",
    "print(\"training dataset\")\n",
    "lm.predict(x=lm.x_train, y=lm.y_train)\n",
    "print(\"test dataset\")\n",
    "lm.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
